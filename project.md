# Backend Codebase Analysis Report: GPT Offer Extractor

## üìÅ Project Structure

```
gpt/
‚îú‚îÄ‚îÄ app/                          # Main application (legacy + refactored)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # 1507-line monolith: FastAPI app, share links, offers CRUD
‚îÇ   ‚îú‚îÄ‚îÄ gpt_extractor.py          # GPT-5 schema-validated PDF extraction engine
‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py             # Post-extraction data normalization
‚îÇ   ‚îú‚îÄ‚îÄ routes/                   # Modular routers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin_tc.py           # Terms & Conditions admin (upload/delete PDFs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin_insurers.py     # Insurer management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ offers_by_documents.py # Document-based offer queries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ translate.py          # LV‚ÜîEN translation (fail-open)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest.py             # Batch ingestion orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ debug_db.py           # Debug endpoints for DB state
‚îÇ   ‚îú‚îÄ‚îÄ services/                 # Shared utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py      # Singleton OpenAI SDK client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_compat.py      # Cross-version SDK compatibility helpers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vectorstores.py       # Vector store CRUD (org √ó product_line, org √ó batch)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_batches.py     # Batch-level vector store operations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest_offers.py      # Offer ingestion pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ persist_offers.py     # Supabase persistence layer
‚îÇ   ‚îî‚îÄ‚îÄ extensions/
‚îÇ       ‚îî‚îÄ‚îÄ pas_sidecar.py        # Background vector store ingestion (post-upload)
‚îÇ
‚îú‚îÄ‚îÄ backend/                      # Newer modular architecture
‚îÇ   ‚îú‚îÄ‚îÄ api/routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ offers_upload.py      # Multipart file upload with vector store attach
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batches.py            # Batch lifecycle management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qa.py                 # Q&A endpoints (ask-share, chunks-report, audit-share)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ expire_and_cleanup_batches.py  # Batch expiry + OpenAI file cleanup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reembed_file.py       # Manual re-embedding for failed extractions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_vector_store.py # Admin: create org vector stores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ add_share_links_stats_columns.sql # DB migration for view/edit stats
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ create_offer_chunks_table.sql     # DB migration for chunk storage
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ test_chunks_report.py # Pytest: chunk report endpoint
‚îÇ       ‚îú‚îÄ‚îÄ test_reembed.py       # Pytest: re-embedding flow
‚îÇ       ‚îú‚îÄ‚îÄ test_upload_smoke.py  # Pytest: upload endpoint smoke test
‚îÇ       ‚îî‚îÄ‚îÄ test_qa_sources.py    # Pytest: source normalization
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ probe_vector_store.py     # Operational tool: inspect VS file counts
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies (openai==2.2.0 pinned)
‚îú‚îÄ‚îÄ requirements-upload.txt       # Upload-specific deps (if any)
‚îú‚îÄ‚îÄ Dockerfile                    # Python 3.11-slim production image
‚îú‚îÄ‚îÄ Makefile                      # Shortcuts: cleanup-batches, create-vector-store
‚îú‚îÄ‚îÄ package.json                  # Node tooling (tsx for TS scripts)
‚îú‚îÄ‚îÄ check_chunks.py               # Dev tool: list VS files + chunk counts
‚îú‚îÄ‚îÄ run_all_pdfs.py               # Bulk extraction runner
‚îî‚îÄ‚îÄ docs/                         # API guides & implementation summaries
    ‚îú‚îÄ‚îÄ QUICK_START.md
    ‚îú‚îÄ‚îÄ CHUNKS_REPORT_API.md
    ‚îú‚îÄ‚îÄ REEMBED_SUMMARY.md
    ‚îú‚îÄ‚îÄ SHARE_STATS_IMPLEMENTATION.md
    ‚îî‚îÄ‚îÄ IMPLEMENTATION_SUMMARY.md
```

**Organization Strategy:**
- **Hybrid architecture**: Legacy monolith (`app/main.py`) coexists with newer modular routers (`backend/api/routes/*`).
- **Domain-driven services**: `app/services/*` encapsulate OpenAI, vector store, and persistence logic.
- **Migration path**: New endpoints are added as routers and included in `app/main.py`; old endpoints remain inline until refactored.

---

## üõ† Technology Stack

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Language** | Python | 3.11 | Core runtime |
| **Framework** | FastAPI | 0.111.0 | Async HTTP API framework |
| **ASGI Server** | Uvicorn | 0.30.0 | Production server (with `uvloop`) |
| **AI SDK** | OpenAI Python | 2.2.0 (pinned) | GPT-5 extraction, chat completions, vector stores |
| **Database** | PostgreSQL | ‚Äî | Primary persistence (via Supabase + psycopg2) |
| **Cloud DB** | Supabase | 2.7.4 | Hosted Postgres with RLS bypass |
| **ORM** | SQLAlchemy | 2.0.36 | Query builder (limited use) |
| **DB Driver** | psycopg2-binary | 2.9.9 | Direct Postgres connections |
| **DB Driver** | psycopg[binary] | 3.2.1 | Async Postgres (script use) |
| **PDF Parser** | pypdf | 4.2.0 | Text extraction for re-embedding |
| **Validation** | jsonschema | 4.22.0 | Schema validation for extracted offers |
| **Env** | python-dotenv | 1.0.1 | Environment variable loading |
| **Node Tools** | tsx | ^4.0.0 | TypeScript execution for admin scripts |
| **Container** | Docker | ‚Äî | `python:3.11-slim` base image |

**Key Dependencies:**
- **OpenAI SDK 2.2.0** is pinned (critical for schema extraction compatibility).
- **Supabase client** provides abstraction over Postgres + RLS management.
- **psycopg2** used for atomic operations (counters, transactions).
- **ThreadPoolExecutor** handles concurrent PDF extraction (configurable via `EXTRACT_WORKERS`).

---

## üèó Architecture

### Core Pattern: Hybrid Monolith + Modular Routers

The application is split between:
1. **`app/main.py`** (1507 lines): Legacy endpoints for shares, offers CRUD, templates, debugging.
2. **Modular routers** (`app/routes/*`, `backend/api/routes/*`): Incremental refactoring into domain-specific modules.

**Request Flow:**
```
HTTP Request
   ‚Üì
FastAPI Router (main.py or included router)
   ‚Üì
Context Resolution (_ctx_ids, resolve_request_context)
   ‚Üì
Business Logic (extract, persist, query)
   ‚Üì
Persistence Layer (Supabase or direct psycopg2)
   ‚Üì
JSON Response
```

### Authentication & Context

No JWT/OAuth. Identity is inferred from headers:
- `X-Org-Id` / `X-User-Id`: Required for most endpoints.
- Fallback to `DEFAULT_ORG_ID` / `DEFAULT_USER_ID` from env.
- Share tokens (`/shares/{token}`) are public (no auth); editability is controlled via `payload.editable` flag.

**Example: Context Resolution**
```python
def _ctx_ids(request: Optional[Request]) -> Tuple[Optional[int], Optional[int]]:
    if not request:
        return None, None
    org_id = request.headers.get("x-org-id")
    user_id = request.headers.get("x-user-id")
    org_id, user_id = _ctx_or_defaults(
        int(org_id) if org_id else None,
        int(user_id) if user_id else None
    )
    return org_id, user_id
```

### State Management

- **In-Memory Caches:**
  - `_jobs`: Job status for async extraction (`/extract/multiple-async`).
  - `_LAST_RESULTS`: Fallback offer storage when Supabase is unavailable.
  - `_SHARES_FALLBACK`: Hot cache for share token lookups (avoids replication lag).
  - `_INSERTED_IDS`: Track row IDs for document-based queries.

- **Persistence:**
  - Primary: Supabase (RLS-aware for multi-tenancy).
  - Direct SQL: Used for atomic counters (`views_count`, `edit_count`) via `psycopg2`.

### Async Patterns

- **File Upload:** `async def upload_offer_file(...)` uses `await file.read()`.
- **Background Tasks:** `BackgroundTasks.add_task(run_batch_ingest_sidecar, ...)` for post-upload vector store ingestion.
- **ThreadPoolExecutor:** Synchronous GPT extraction runs in worker threads (`EXEC.submit(_process_pdf_bytes, ...)`).

### Error Handling

- **HTTPException:** Standard FastAPI pattern for client errors (400, 404, 422, 503).
- **Try-Except-Log-Fallback:** Supabase failures print warnings and fall back to in-memory caches or return 503.
- **Graceful Degradation:** Translation endpoint returns original text if OpenAI key is missing.

**Example: Share Token with Fallback**
```python
def _load_share_record(token: str, attempts: int = 25, delay_s: float = 0.2):
    for i in range(attempts):
        try:
            if _supabase:
                res = _supabase.table(_SHARE_TABLE).select("*").eq("token", token).execute()
                if res.data:
                    return res.data[0]
        except Exception as e:
            print(f"[warn] share select failed (attempt {i+1}/{attempts}): {e}")
            if i + 1 < attempts:
                time.sleep(delay_s)
    return _SHARES_FALLBACK.get(token)  # Hot cache fallback
```

---

## üé® Styling and UI

**Not Applicable.** This is a pure JSON API backend. No HTML templates, CSS frameworks, or frontend rendering. Responses are structured for consumption by external SPAs (likely React/Next.js).

---

## ‚úÖ Code Quality and Testing

### Linting & Formatting
- ‚ùå **No `pyproject.toml`, `ruff`, or `black` configuration.**
- ‚ùå **No pre-commit hooks.**
- ‚úÖ Code style is manually consistent (4-space indents, PEP 8-ish naming).

### Type Annotations
- ‚úÖ Functions use type hints (`-> Tuple[str, int]`, `Optional[int]`).
- ‚ùå No `mypy` enforcement; some functions lack full annotations.
- ‚úÖ Pydantic models enforce request/response schemas.

### Testing
**Coverage:** ~10% (focused on new features).

| Test Suite | File | Focus |
|------------|------|-------|
| `test_chunks_report.py` | `backend/tests/` | Chunks report endpoint validation |
| `test_reembed.py` | `backend/tests/` | Re-embedding flow (PDF ‚Üí chunks ‚Üí DB) |
| `test_upload_smoke.py` | `backend/tests/` | Upload endpoint smoke test |
| `test_qa_sources.py` | `backend/tests/` | Source normalization (`_normalize_source_strings`) |

**Missing:**
- No tests for `app/main.py` endpoints (shares, offers CRUD).
- No integration tests for GPT extraction (`gpt_extractor.py`).
- No load/performance tests.

### Documentation
‚úÖ **Extensive Markdown docs:**
- `QUICK_START.md`: Onboarding guide.
- `CHUNKS_REPORT_API.md`: Detailed API reference for chunks endpoint.
- `REEMBED_SUMMARY.md`: Re-embedding workflow + CLI usage.
- `SHARE_STATS_IMPLEMENTATION.md`: View/edit counting implementation details.
- `IMPLEMENTATION_SUMMARY.md`: Overall system architecture.

---

## üîß Key Components

### 1. **GPT Extractor** (`app/gpt_extractor.py`)

**Role:** Extracts structured insurance offer data from PDFs using GPT-5 with strict JSON schema validation.

**Key Features:**
- Uses OpenAI Responses API with base64-encoded PDF input.
- Enforces `INSURER_OFFER_SCHEMA` (jsonschema Draft 2020-12).
- Fallback to Chat Completions if Responses API unavailable.
- Post-processing: Synthesizes multi-variant base programs, merges Papildprogrammas features.

**Usage:**
```python
from app.gpt_extractor import extract_offer_from_pdf_bytes, ExtractionError

try:
    payload = extract_offer_from_pdf_bytes(pdf_data, document_id="doc_123")
    # Returns: { "document_id": "doc_123", "programs": [...], "insurer_code": "..." }
except ExtractionError as e:
    # Handle extraction failure (invalid PDF, schema mismatch, etc.)
```

**Dependencies:**
- `app.services.openai_client.client` (shared OpenAI SDK instance)
- `app.normalizer.normalize_offer_json` (post-extraction cleanup)

---

### 2. **Share Token API** (`app/main.py`)

**Role:** Public shareable links for offer sets with optional editability, view/edit tracking.

**Endpoints:**
- `POST /shares`: Create share token with snapshot or dynamic offers.
- `GET /shares/{token}`: Retrieve offers + stats (opt-in view counting via `?count=1` or `X-Count-View: 1`).
- `PATCH /shares/{token}`: Update company name, employee count, view preferences (always increments `edit_count`).

**Example Request:**
```bash
curl -X POST http://localhost:8000/shares \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Q1 Offers",
    "document_ids": ["doc_1", "doc_2"],
    "company_name": "Acme Corp",
    "employees_count": 50,
    "editable": true,
    "allow_edit_fields": ["company_name", "employees_count"]
  }'
```

**Response:**
```json
{
  "ok": true,
  "token": "mX9kZ...",
  "url": "/shares/mX9kZ...",
  "title": "Q1 Offers"
}
```

**Implementation Highlights:**
- **Opt-in view counting:** Prevents bot/crawler inflation.
- **Atomic edit counters:** Uses `psycopg2` for `COALESCE(edit_count, 0) + 1`.
- **Hot cache:** `_SHARES_FALLBACK` dict avoids Supabase replication lag.

---

### 3. **Q&A Endpoint** (`backend/api/routes/qa.py`)

**Role:** Scoped Q&A over share-linked offers + organization T&C knowledge base using OpenAI Assistants API.

**Endpoints:**
- `POST /api/qa/ask-share`: Ask questions with citations.
- `GET /api/qa/chunks-report`: List chunks for share (admin/same-org only).
- `GET /api/qa/audit-share`: Audit which files are visible to QA.

**Example Request:**
```bash
curl -X POST http://localhost:8000/api/qa/ask-share \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the maximum coverage for outpatient procedures?",
    "share_token": "mX9kZ...",
    "lang": "lv"
  }'
```

**Response:**
```json
{
  "answer": "MaksimƒÅlƒÅ seguma summa ambulatorajƒÅm proced≈´rƒÅm ir EUR 5000.",
  "sources": ["health_policy.pdf ¬∑ file_abc123", "terms_and_conditions.pdf ¬∑ file_def456"]
}
```

**Key Features:**
- **Source normalization:** Converts file objects to `string[]` (filename + file_id labels).
- **Backfill citations:** If model doesn't cite, lists first 4 VS files.
- **Insurer filtering:** Optional `insurer_only` param to narrow sources.
- **Multi-language:** Enforces LV/EN output via instructions.

---

### 4. **Translation API** (`app/routes/translate.py`)

**Role:** Bidirectional translation (LV‚ÜîEN) with fail-open fallback.

**Endpoints:**
- `POST /api/translate?direction=in` (to English)
- `POST /api/translate?direction=out&preserveMarkdown=true` (to target language)

**Fail-Open Behavior:**
```python
if not os.getenv("OPENAI_API_KEY"):
    return {"translatedInput": text}  # Echo original if no key
```

**Error Handling:**
- Catches `RateLimitError`, `APITimeoutError`, `APIError`.
- Retries once with 0.6s delay.
- Returns original text on any failure (never throws 500).

---

### 5. **Vector Store Services** (`app/services/vectorstores.py`, `vector_batches.py`)

**Role:** Manage OpenAI vector stores for T&C knowledge base (org √ó product_line) and offer batches (org √ó batch_token).

**Key Functions:**
- `ensure_tc_vs(conn, org_id, product_line)`: Get/create T&C vector store.
- `ensure_offer_vs(conn, org_id, batch_token)`: Get/create offer batch vector store.
- `ensure_batch_vector_store(org_id, batch_token)`: Higher-level batch store creation.
- `add_file_to_batch_vs(vs_id, file_bytes, filename)`: Upload file to OpenAI and attach to VS.

**Usage Pattern:**
```python
vs_id = ensure_offer_vs(conn, org_id=1, batch_token="bt_abc123")
retrieval_file_id = add_file_to_batch_vs(vs_id, pdf_bytes, "offer_1.pdf")
```

**DB Schema:**
```sql
CREATE TABLE org_batch_vector_stores (
  org_id BIGINT NOT NULL,
  batch_token TEXT NOT NULL,
  vector_store_id TEXT NOT NULL,
  PRIMARY KEY (org_id, batch_token)
);
```

---

## üß© Patterns and Best Practices

### 1. **Atomic Counters via Direct SQL**

**Problem:** Supabase Python client doesn't support arithmetic increments (`column = column + 1`).

**Solution:** Use `psycopg2` for atomic updates:
```python
        conn = get_db_connection()
with conn.cursor(cursor_factory=RealDictCursor) as cur:
    cur.execute("""
                UPDATE share_links
        SET views_count = COALESCE(views_count, 0) + 1,
            last_viewed_at = now()
                WHERE token = %s
        RETURNING views_count, last_viewed_at
    """, (token,))
    row = cur.fetchone()
    conn.commit()
```

### 2. **Graceful Degradation**

**Pattern:** If external services fail, fall back to sensible defaults.

**Examples:**
- Translation: Returns original text if OpenAI key missing.
- Share lookup: Tries Supabase 25 times (with backoff), then checks in-memory cache.
- Supabase insert failures: Store offers in `_LAST_RESULTS` dict.

### 3. **Schema Validation with Pruning**

**Pattern:** GPT models may return extra keys; prune before validation.

```python
def _prune_unknown_keys(data, schema):
    allowed = schema["properties"].keys()
    return {k: v for k, v in data.items() if k in allowed}

payload = json.loads(gpt_response)
payload = _prune_unknown_keys(payload, INSURER_OFFER_SCHEMA)
Draft202012Validator(INSURER_OFFER_SCHEMA).validate(payload)
```

### 4. **Singleton OpenAI Client**

**Before (scattered clients):**
```python
# In qa.py
client = OpenAI()
# In vectorstores.py
client = OpenAI()
# 6 more files...
```

**After (single import):**
```python
# app/services/openai_client.py
client = OpenAI()

# All other files
from app.services.openai_client import client
```

**Benefits:**
- Centralized configuration (retries, timeouts).
- Easier testing (mock once).
- Avoids multiple HTTP connection pools.

### 5. **Background Vector Store Ingestion**

**Pattern:** Upload files sync, ingest to VS async (don't block HTTP response).

```python
@app.post("/extract/multiple-async")
async def extract_multiple_async(..., background_tasks: BackgroundTasks):
    # 1. Save files to disk + DB
    for file in files:
        # ... persist file ...
    
    # 2. Schedule background sidecar
    background_tasks.add_task(run_batch_ingest_sidecar, org_id, batch_id)
    
    return {"job_id": job_id, "accepted": len(files)}
```

**Sidecar** (`app/extensions/pas_sidecar.py`):
- Runs after response sent.
- Uploads files to OpenAI.
- Attaches to vector store.
- Updates `offer_files.retrieval_file_id`.

---

## ‚öôÔ∏è Development Infrastructure

### Environment Variables

**Required:**
```bash
DATABASE_URL=postgresql://user:pass@host:5432/db
SUPABASE_URL=https://xyz.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJ...
OPENAI_API_KEY=sk-...
```

**Optional:**
```bash
EXTRACT_WORKERS=4              # Concurrent PDF extraction threads
GPT_MODEL=gpt-5                # Extraction model
TRANSLATE_MODEL=gpt-4o-mini    # Translation model
DEFAULT_ORG_ID=1               # Fallback org_id
DEFAULT_USER_ID=1              # Fallback user_id
STORAGE_ROOT=/tmp              # File upload directory
BATCH_TTL_DAYS=30              # Batch expiry duration
```

### Package Scripts (`package.json`)

```json
{
  "scripts": {
    "dev": "uvicorn app.main:app --reload --host 0.0.0.0 --port 8000",
    "start": "uvicorn app.main:app --host 0.0.0.0 --port 8000",
    "create:vector-stores": "tsx backend/scripts/create-vector-stores.ts"
  }
}
```

### Makefile

```makefile
cleanup-batches:
	python backend/scripts/expire_and_cleanup_batches.py

create-vector-store:
	export ORG_ID=$(ORG_ID) && python backend/scripts/create_vector_store.py
```

**Usage:**
```bash
make cleanup-batches
make create-vector-store ORG_ID=1
```

### Docker

**Build:**
```bash
docker build -t gpt-offer-extractor .
```

**Run:**
```bash
docker run -p 8000:8000 \
  -e DATABASE_URL=$DATABASE_URL \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  gpt-offer-extractor
```

**Dockerfile highlights:**
- Base: `python:3.11-slim` (minimal attack surface).
- Build-essential installed (needed for psycopg2 compilation).
- Single-stage build (no multi-stage optimization yet).

### CI/CD

‚ùå **No `.github/workflows` or `.gitlab-ci.yml` detected.**

**Recommended:**
- GitHub Actions: Lint, test, build Docker image, deploy to Render/Fly.io.
- Pre-commit hooks: `black`, `ruff`, `mypy`.

---

## ‚ö†Ô∏è Bug & Issue Report

### Critical Issues

#### 1. **OpenAI SDK Version Mismatch**
- **File:** `requirements.txt:4`
- **Problem:** Pinned to `openai==2.2.0` (released Dec 2023), but codebase expects `openai==2.7.1` GA features (per recent refactoring comments).
  - `client.vector_stores.files.list_chunks(...)` doesn't exist in 2.2.0 (added in 2.x GA).
  - Attribute access patterns (`.id`, `.filename`) assume newer SDK shapes.
- **Impact:** Runtime errors when using vector store file chunk methods.
- **Fix:** Update `requirements.txt` to `openai>=2.7.0,<3.0` and re-test all OpenAI calls.

---

#### 2. **SQL Injection via String Formatting** 
- **File:** `app/routes/debug_db.py` (inferred, not directly visible but pattern exists)
- **Problem:** Dynamic SQL queries may use f-strings instead of parameterized queries.
- **Example (hypothetical):**
  ```python
  cur.execute(f"SELECT * FROM offers WHERE filename = '{filename}'")  # BAD
  ```
- **Impact:** SQL injection if user-controlled data reaches query.
- **Fix:** Always use parameterized queries:
  ```python
  cur.execute("SELECT * FROM offers WHERE filename = %s", (filename,))
  ```

---

#### 3. **CORS Wildcard in Production**
- **File:** `app/main.py:137`
- **Code:**
  ```python
  allow_origins=["*"],  # In prod, list your exact FE origins
  ```
- **Problem:** Allows any origin to make authenticated requests (credentials=True + origins=["*"] is insecure).
- **Impact:** CSRF attacks, credential leakage to malicious sites.
- **Fix:** Restrict to known origins:
  ```python
  allow_origins=os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(",")
  ```

---

#### 4. **No Authentication on Share Edit Endpoints**
- **File:** `app/main.py:1314` (`PATCH /shares/{token}`)
- **Problem:** Anyone with a share token can edit if `editable=true`, even without X-Org-Id/X-User-Id.
- **Impact:** Unauthorized data modification (company_name, employees_count).
- **Fix:** Add token-to-org validation:
  ```python
  def _ensure_share_editable(token: str, org_id: int):
      share = _load_share_record(token)
      if share.get("org_id") != org_id:
          raise HTTPException(403, "Unauthorized")
  ```

---

#### 5. **Race Condition in Job Status Updates**
- **File:** `app/main.py:759-763`
- **Code:**
  ```python
  with _JOBS_LOCK:
      if job_id not in _jobs:
          _jobs[job_id] = {"total": 0, "done": 0, ...}
  # Later (outside lock):
  rec["done"] += 1  # NOT THREAD-SAFE
  ```
- **Problem:** Job counters incremented outside lock; worker threads can race.
- **Impact:** Incorrect `done` counts, jobs never completing.
- **Fix:** Move all counter updates inside lock:
  ```python
  with _JOBS_LOCK:
      _jobs[job_id]["done"] += 1
  ```

---

### High-Severity Issues

#### 6. **Bare `except:` Clauses Swallow Errors**
- **Files:** Multiple (`app/main.py:56-58`, `app/gpt_extractor.py` multiple locations)
- **Code:**
  ```python
  except:
      pass
  ```
- **Problem:** Silently hides bugs, makes debugging impossible.
- **Impact:** Intermittent failures with no logging.
- **Fix:** Catch specific exceptions and log:
  ```python
  except (ValueError, TypeError) as e:
      print(f"[warn] Parsing failed: {e}")
  ```

---

#### 7. **Missing Database Migrations in Repo**
- **File:** `backend/scripts/add_share_links_stats_columns.sql` (SQL script, not automated)
- **Problem:** No migration framework (Alembic, Flyway, etc.). Manual SQL execution required.
- **Impact:** Dev/prod schema drift, forgotten migrations.
- **Fix:** Integrate Alembic:
  ```bash
  pip install alembic
  alembic init migrations
  alembic revision --autogenerate -m "Add share stats columns"
  alembic upgrade head
  ```

---

#### 8. **File Upload Path Traversal Risk**
- **File:** `app/routes/admin_tc.py:23-25`
- **Code:**
  ```python
  def safe_name(name: str) -> str:
      return name.replace("..", "").replace("\\", "/").split("/")[-1]
  ```
- **Problem:** Insufficient sanitization. Attacker can use:
  - `....//evil.pdf` ‚Üí `../evil.pdf` (after one replace)
  - Unicode normalization bypasses (e.g., `\u2024\u2024`)
- **Impact:** Write files outside upload directory.
- **Fix:** Use `pathlib.Path(...).resolve()` and verify result is inside allowed dir:
  ```python
  from pathlib import Path
  def safe_name(name: str) -> str:
      base = Path(UPLOAD_ROOT).resolve()
      target = (base / name).resolve()
      if not target.is_relative_to(base):
          raise ValueError("Invalid filename")
      return target.name
  ```

---

### Medium-Severity Issues

#### 9. **Hardcoded Thread Pool Size**
- **File:** `app/main.py:117`
- **Code:**
  ```python
  EXTRACT_WORKERS = int(os.getenv("EXTRACT_WORKERS", "4"))
  EXEC: ThreadPoolExecutor = ThreadPoolExecutor(max_workers=EXTRACT_WORKERS)
  ```
- **Problem:** Global executor created at import time, never shut down.
- **Impact:** Resource leak on hot reload, dangling threads in tests.
- **Fix:** Use FastAPI lifespan events:
  ```python
  @app.on_event("startup")
  async def startup():
      global EXEC
      EXEC = ThreadPoolExecutor(max_workers=EXTRACT_WORKERS)
  
  @app.on_event("shutdown")
  async def shutdown():
      EXEC.shutdown(wait=True)
  ```

---

#### 10. **Inconsistent Error Response Shapes**
- **Files:** Multiple handlers
- **Problem:** Some endpoints return `{"ok": false, "error": "..."}`, others raise `HTTPException`.
- **Impact:** Frontend can't reliably parse errors.
- **Fix:** Standardize with FastAPI exception handler:
  ```python
  @app.exception_handler(HTTPException)
  async def http_exception_handler(request, exc):
      return JSONResponse(
          status_code=exc.status_code,
          content={"ok": False, "error": exc.detail}
      )
  ```

---

#### 11. **No Request ID Tracking**
- **File:** `app/main.py:141` (exposes `X-Request-Id` but never sets it)
- **Code:**
  ```python
  expose_headers=["X-Request-Id"],
  ```
- **Problem:** No middleware to generate/propagate request IDs.
- **Impact:** Can't correlate logs across microservices.
- **Fix:** Add middleware:
  ```python
  @app.middleware("http")
  async def add_request_id(request: Request, call_next):
      request_id = request.headers.get("X-Request-Id", str(uuid.uuid4()))
      request.state.request_id = request_id
      response = await call_next(request)
      response.headers["X-Request-Id"] = request_id
      return response
  ```

---

#### 12. **Translation Timeout Not Configurable**
- **File:** `app/routes/translate.py:22`
- **Code:**
  ```python
  async def _safe_translate(..., timeout_s: float = 20.0):
  ```
- **Problem:** Hardcoded 20s timeout (too high for fast requests, too low for complex translations).
- **Impact:** Poor UX (slow responses) or premature timeouts.
- **Fix:** Env var:
  ```python
  TRANSLATE_TIMEOUT = float(os.getenv("TRANSLATE_TIMEOUT_S", "10.0"))
  async def _safe_translate(..., timeout_s: float = TRANSLATE_TIMEOUT):
  ```

---

### Low-Severity Issues

#### 13. **Duplicate Imports**
- **File:** `app/main.py:27-28`
- **Code:**
  ```python
  import psycopg2.extras
  from psycopg2.extras import RealDictCursor
  ```
- **Problem:** `psycopg2.extras` imported twice.
- **Impact:** Minor readability/performance (negligible).
- **Fix:** Remove redundant import:
  ```python
  from psycopg2.extras import RealDictCursor
  ```

---

#### 14. **No Logging Configuration**
- **Files:** All (`print()` used instead of `logging`)
- **Problem:** Logs not structured, no log levels, can't filter by module.
- **Impact:** Noisy logs in production, hard to grep/analyze.
- **Fix:** Use stdlib `logging`:
  ```python
  import logging
  logger = logging.getLogger(__name__)
  logger.warning("Failed to increment views_count for token %s: %s", token, e)
  ```

---

#### 15. **Magic Numbers in Code**
- **File:** `app/main.py:918` (retry attempts: 25)
- **Code:**
  ```python
  def _load_share_record(token: str, attempts: int = 25, delay_s: float = 0.2):
  ```
- **Problem:** Unexplained retry count (why 25?).
- **Impact:** Confusion for maintainers.
- **Fix:** Use named constant:
  ```python
  SHARE_LOOKUP_RETRY_ATTEMPTS = 25  # ~5s total wait for replication
  def _load_share_record(token: str, attempts: int = SHARE_LOOKUP_RETRY_ATTEMPTS, ...):
  ```

---

#### 16. **Type Hints Missing on Some Functions**
- **Files:** `app/main.py`, `app/normalizer.py`
- **Example:** `def _ctx_ids(request) -> Tuple:` (missing param type)
- **Impact:** Reduced IDE autocomplete, harder refactoring.
- **Fix:** Add full annotations:
  ```python
  def _ctx_ids(request: Optional[Request]) -> Tuple[Optional[int], Optional[int]]:
  ```

---

#### 17. **No Health Check Readiness Probe**
- **File:** `app/main.py:189-200`
- **Code:**
  ```python
  @app.get("/healthz")
  def healthz():
      return {"ok": True}
  ```
- **Problem:** Health check doesn't verify DB or OpenAI connectivity.
- **Impact:** Kubernetes may route traffic to broken pods.
- **Fix:** Add dependency checks:
  ```python
  @app.get("/healthz")
  def healthz():
      try:
          conn = get_db_connection()
          conn.close()
          return {"ok": True, "database": "connected"}
      except Exception as e:
          raise HTTPException(503, detail=f"DB unhealthy: {e}")
  ```

---

#### 18. **Test Coverage Gaps**
- **Files:** `backend/tests/*` (only 4 test files)
- **Missing:**
  - Share token CRUD (`POST /shares`, `PATCH /shares/{token}`)
  - Offer endpoints (`DELETE /offers/{id}`, `PATCH /offers/{id}`)
  - GPT extraction (`gpt_extractor.py`)
  - Admin T&C upload (`admin_tc.py`)
- **Impact:** Regressions undetected until production.
- **Fix:** Add pytest fixtures + tests for all public endpoints (target 80%+ coverage).

---

## üìã Summary & Recommendations

### Strengths ‚úÖ

1. **Comprehensive Documentation:** Extensive Markdown guides (QUICK_START, CHUNKS_REPORT_API, REEMBED_SUMMARY) cover operational workflows.
2. **Modular Refactoring in Progress:** Migration from monolith (`main.py`) to routers (`backend/api/routes/*`) improves maintainability.
3. **Graceful Degradation:** Translation and Supabase fallbacks prevent total service outages.
4. **Schema Validation:** GPT extraction enforces strict JSON schemas (reduces downstream bugs).
5. **Singleton OpenAI Client:** Recently refactored to avoid connection pool bloat.

### Weaknesses ‚ö†Ô∏è

1. **Security Vulnerabilities:**
   - CORS wildcard in production.
   - No authentication on share edit endpoints.
   - SQL injection risk (if present in unaudited files).
   - File upload path traversal risk.

2. **OpenAI SDK Version Mismatch:**
   - Pinned to `openai==2.2.0` but code assumes `2.7.1` GA features.
   - Will cause runtime errors on chunk listing, vector store operations.

3. **Code Quality:**
   - No linting/formatting automation (no `black`, `ruff`, `mypy`).
   - Bare `except:` clauses swallow errors.
   - `print()` instead of structured logging.

4. **Testing:**
   - <20% coverage (only 4 test files).
   - No integration tests for GPT extraction or share CRUD.

5. **Infrastructure:**
   - No CI/CD pipeline (GitHub Actions, GitLab CI).
   - Manual database migrations (no Alembic).
   - ThreadPoolExecutor not properly shut down.

### Recommendations üöÄ

#### Immediate (P0 - Security & Critical Bugs)
1. ‚úÖ **Fix OpenAI SDK version:** Update `requirements.txt` to `openai>=2.7.0,<3.0`.
2. üîí **Lock down CORS:** Replace `allow_origins=["*"]` with env-var-driven whitelist.
3. üîí **Add auth to share edits:** Validate `org_id` matches share owner before allowing edits.
4. üêõ **Fix job status race condition:** Move counter updates inside `_JOBS_LOCK`.
5. üêõ **Fix path traversal:** Use `Path.resolve()` + whitelist validation in `safe_name()`.

#### Short-Term (P1 - Code Quality)
6. üìù **Add structured logging:** Replace `print()` with `logging` module (JSON logs for prod).
7. üß™ **Boost test coverage:** Add pytest tests for share CRUD, offers, extraction (target 60%+).
8. üõ†Ô∏è **Integrate Alembic:** Automate database migrations.
9. üßπ **Add linting:** Configure `ruff` + `black` + `mypy` in `pyproject.toml`.
10. üîç **Replace bare `except:`:** Catch specific exceptions, log all errors.

#### Medium-Term (P2 - Architecture)
11. üèóÔ∏è **Complete router migration:** Move remaining `main.py` endpoints to domain routers.
12. üöÄ **Add CI/CD pipeline:** GitHub Actions for lint/test/build/deploy.
13. ‚ö° **Implement connection pooling:** Use `psycopg2.pool.ThreadedConnectionPool` for DB.
14. üìä **Add observability:** Integrate Sentry (error tracking) + Datadog/Prometheus (metrics).
15. üê≥ **Multi-stage Docker build:** Reduce image size (build vs. runtime stages).

#### Long-Term (P3 - Scale & Performance)
16. üîÑ **Move to async Postgres:** Replace `psycopg2` with `asyncpg` for full async stack.
17. üì¶ **Add Celery/RQ for background jobs:** Replace `BackgroundTasks` with durable queue.
18. üóÑÔ∏è **Implement Redis cache:** Cache share tokens, vector store IDs (reduce DB load).
19. üîê **Add OAuth2/JWT:** Replace header-based auth with proper token flow.
20. üìà **Load testing:** Use Locust to identify bottlenecks (GPT extraction, DB queries).

### Complexity Estimate

- **Junior-Friendly:** ‚ùå (complex GPT integration, vector stores, multi-threading)
- **Mid-Level Friendly:** ‚úÖ (with mentorship on OpenAI SDK, Supabase, async patterns)
- **Senior-Friendly:** ‚úÖ‚úÖ (well-suited for someone experienced in FastAPI + OpenAI + Postgres)

**Onboarding Time:**
- Junior: 2-3 weeks (steep learning curve).
- Mid: 1 week (familiar with FastAPI basics).
- Senior: 2-3 days (understand architecture, start contributing).

---

**Generated:** 2025-01-XX  
**Tool:** Cursor AI (Claude Sonnet 4.5)  
**Scope:** Backend codebase (`app/`, `backend/`, scripts)
