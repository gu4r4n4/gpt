

project.md
+75
-1630

# Backend Codebase Analysis Report: GPT Offer Extractor

**Project**: Insurance Offer PDF Extraction & Analysis System  
**Type**: FastAPI Backend Service with AI/ML Integration  
**Version**: 1.0.0  
**Analysis Date**: October 26, 2025

---

## üìÅ Project Structure
- `backend/` ‚Äì FastAPI backend focused on offer ingestion, Q&A, and maintenance scripts. Organized by concern (API routes, scripts, tests).„ÄêF:backend/api/routes/offers_upload.py‚Ä†L1-L592„Äë„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L1-L200„Äë
  - `backend/api/routes/` ‚Äì HTTP endpoints for batch management, uploads, and question answering, each encapsulated in its own module and FastAPI router.„ÄêF:backend/api/routes/batches.py‚Ä†L1-L82„Äë„ÄêF:backend/api/routes/qa.py‚Ä†L1-L608„Äë
  - `backend/scripts/` ‚Äì Operational scripts for vector store management and cleanup across Python and TypeScript tooling.„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L1-L200„Äë„ÄêF:backend/scripts/create-vector-stores.ts‚Ä†L1-L91„Äë
  - `backend/tests/` ‚Äì Pytest suites covering upload flows, re-embedding logic, and share chunk reporting helpers.„ÄêF:backend/tests/test_upload_smoke.py‚Ä†L1-L109„Äë„ÄêF:backend/tests/test_reembed.py‚Ä†L1-L177„Äë
- `app/services/` ‚Äì Shared service layer for OpenAI/vector store orchestration used by API routes.„ÄêF:app/services/vectorstores.py‚Ä†L1-L169„Äë„ÄêF:app/services/openai_compat.py‚Ä†L1-L40„Äë
- Root config ‚Äì Makefile, `requirements.txt`, and `package.json` provide hybrid Python/TypeScript tooling and scripts for local ops and deployment.„ÄêF:Makefile‚Ä†L1-L9„Äë„ÄêF:requirements.txt‚Ä†L1-L10„Äë„ÄêF:package.json‚Ä†L1-L21„Äë

```
e:\FAILI\1.OnGo\1.AGENT\v2\be\gpt\
‚îú‚îÄ‚îÄ app/                          # Main application package (legacy/v1 API)
‚îÇ   ‚îú‚îÄ‚îÄ extensions/              # Extension modules (vector store sidecar)
‚îÇ   ‚îú‚îÄ‚îÄ routes/                  # API route modules (v1)
‚îÇ   ‚îú‚îÄ‚îÄ services/                # Business logic services
‚îÇ   ‚îú‚îÄ‚îÄ gpt_extractor.py         # Core PDF extraction logic (977 lines)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # FastAPI application entry point (1,396 lines)
‚îÇ   ‚îî‚îÄ‚îÄ normalizer.py            # Data normalization utilities (259 lines)
‚îú‚îÄ‚îÄ backend/                      # New API structure (v2/modular)
‚îÇ   ‚îú‚îÄ‚îÄ api/routes/              # RESTful API endpoints (v2)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                 # CLI utilities and migrations
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ scripts/                      # Additional utility scripts
‚îú‚îÄ‚îÄ Dockerfile                    # Container configuration
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ Makefile                      # Operational commands
‚îî‚îÄ‚îÄ *.md                          # Documentation files
```

### Directory Purpose

**`app/`** - **Legacy/V1 Application Core** (1,396-line monolithic main.py)
- Primary FastAPI application with offer extraction, batch processing, and share management
- Contains complex PDF extraction pipeline using OpenAI GPT models
- Handles concurrent processing with ThreadPoolExecutor
- Manages in-memory state for job tracking and result caching

**`app/routes/`** - **V1 API Endpoints**
- `admin_insurers.py` - Insurer management (CRUD operations)
- `admin_tc.py` - Terms & conditions management (T&C file uploads)
- `debug_db.py` - Database debugging utilities (18 lines, minimal)
- `ingest.py` - Legacy PDF ingestion endpoint
- `offers_by_documents.py` - Document-based offer queries

**`app/services/`** - **Business Logic Layer**
- `vectorstores.py` - OpenAI vector store management (170 lines)
- `vector_batches.py` - Batch-level vector store operations
- `persist_offers.py` - Database persistence with SQLAlchemy
- `openai_compat.py` - OpenAI API compatibility wrapper
- `ingest_offers.py` - Offer ingestion service logic

**`app/extensions/`** - **Extensibility Modules**
- `pas_sidecar.py` - Post-upload vector store processing sidecar (198 lines)
- Runs asynchronously after v1 uploads to add vector search without breaking contracts

**`backend/`** - **V2 Modular Architecture** (newer, cleaner structure)
- Separates concerns better than legacy app/
- More RESTful API design with proper resource naming

**`backend/api/routes/`** - **V2 API Endpoints**
- `qa.py` - Q&A endpoints with vector search (845 lines, newly expanded)
- `batches.py` - Batch management API (84 lines)
- `offers_upload.py` - File upload with vector store integration (575 lines)

**`backend/scripts/`** - **Operational CLI Tools**
- `reembed_file.py` - Manual chunk re-embedding (400+ lines CLI)
- `create_vector_store.py` - Vector store initialization
- `expire_and_cleanup_batches.py` - Batch lifecycle management
- `create_offer_chunks_table.sql` - Database migration

**`backend/tests/`** - **Test Suite**
- `test_chunks_report.py` - Chunks report endpoint tests
- `test_reembed.py` - Re-embedding functionality tests
- `test_upload_smoke.py` - Upload smoke tests

### Organizational Pattern

**Hybrid Architecture**: The codebase shows evolution from a monolithic structure (`app/main.py`) to a more modular design (`backend/`). This suggests:

1. **Phase 1**: Rapid development in `app/main.py` (all-in-one file)
2. **Phase 2**: Service extraction to `app/services/` and `app/routes/`
3. **Phase 3**: New v2 API in `backend/api/` with better separation

**Current State**: Dual-track development with legacy compatibility

---
The codebase is primarily concern-oriented: API routes by feature, service modules for shared infrastructure, and standalone operational scripts, with supporting tests colocated by domain.

## üõ† Technology Stack

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Framework** | FastAPI | 0.111.0 | High-performance async web framework with automatic OpenAPI docs |
| **Runtime** | Python | 3.11 | Modern Python with type hints and performance improvements |
| **ASGI Server** | Uvicorn | 0.30.0 | Production-grade ASGI server with hot reload |
| **Database** | PostgreSQL | - | Primary relational database (via Supabase) |
| **ORM (Legacy)** | SQLAlchemy | 2.0.36 | Database abstraction for v1 endpoints |
| **DB Adapter** | psycopg2-binary | 2.9.9 | PostgreSQL adapter (synchronous) |
| **DB Adapter (New)** | psycopg[binary] | 3.2.1 | PostgreSQL adapter v3 (async support) |
| **AI/ML** | OpenAI API | 2.2.0 | GPT models for PDF extraction (PINNED version) |
| **PDF Processing** | PyPDF | 4.2.0 | PDF text extraction and parsing |
| **Schema Validation** | JSONSchema | 4.22.0 | Strict data validation against JSON schemas |
| **Environment** | python-dotenv | 1.0.1 | Environment variable management |
| **Cloud DB** | Supabase | 2.7.4 | PostgreSQL-as-a-Service with real-time features |
| **Containerization** | Docker | - | Application containerization (Python 3.11-slim) |
| **JS Runtime** | Node.js | - | For TypeScript scripts (vector store setup) |
| **TS Executor** | tsx | ^4.0.0 | TypeScript execution for scripts |

### Notable Dependency Decisions

**OpenAI API Pinned**: Version `2.2.0` is explicitly pinned with comment "üîí pin here (critical!)" - suggests API stability issues or breaking changes in newer versions.

**Dual psycopg**: Both psycopg2 (v2.9.9) and psycopg[binary] (v3.2.1) are installed, indicating migration in progress from v2 to v3.

**Minimal Dependencies**: Only 10 Python packages - lean and focused.

---
| Layer | Technology & Version | Purpose |
| --- | --- | --- |
| Web framework | FastAPI 0.111 | Defines REST endpoints and dependency injection for backend services.„ÄêF:requirements.txt‚Ä†L1-L2„Äë„ÄêF:backend/api/routes/batches.py‚Ä†L1-L47„Äë|
| ASGI server | Uvicorn 0.30 | Development and production serving entrypoint (`package.json` scripts).„ÄêF:requirements.txt‚Ä†L2-L2„Äë„ÄêF:package.json‚Ä†L6-L13„Äë|
| Database client | psycopg2 / psycopg3 | PostgreSQL connectivity for transactional queries in routes and scripts.„ÄêF:requirements.txt‚Ä†L7-L10„Äë„ÄêF:backend/api/routes/offers_upload.py‚Ä†L24-L183„Äë|
| Vector search | OpenAI Assistants / Vector Stores (openai 2.2) | File upload, retrieval, and embeddings management.„ÄêF:requirements.txt‚Ä†L4-L4„Äë„ÄêF:backend/api/routes/qa.py‚Ä†L81-L148„Äë|
| PDF parsing | pypdf 4.2 | Text extraction for re-embedding flows.„ÄêF:requirements.txt‚Ä†L5-L5„Äë„ÄêF:backend/api/routes/qa.py‚Ä†L1-L200„Äë|
| Cloud storage (optional) | boto3 (implicit) | Used when `S3_BUCKET` is configured for uploads/cleanup (not listed in requirements).„ÄêF:backend/api/routes/offers_upload.py‚Ä†L211-L225„Äë„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L55-L82„Äë|
| Task scripts | tsx, TypeScript, pg | Vector store provisioning via Node tooling.„ÄêF:package.json‚Ä†L6-L20„Äë„ÄêF:backend/scripts/create-vector-stores.ts‚Ä†L1-L91„Äë|
| Testing | pytest | Functional and unit coverage for API helpers.„ÄêF:requirements.txt‚Ä†L1-L10„Äë„ÄêF:backend/tests/test_reembed.py‚Ä†L1-L177„Äë|

## üèó Architecture

### Core Architecture Pattern

**Multi-Strategy AI-Powered PDF Processing Pipeline** with event-driven background workers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Client      ‚îÇ
‚îÇ (FE/API)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FastAPI Application (app/main.py)                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ Multi-Strategy PDF Extraction             ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îÇ 1. Responses API + PDF + Schema      ‚îÇ  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îÇ 2. Responses API + PDF (no schema)   ‚îÇ  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îÇ 3. Chat Completions + Extracted Text ‚îÇ  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ         ‚ñº                                  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Variant Detection & Augmentation     ‚îÇ  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ         ‚ñº                                  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Papildprogrammas Merging             ‚îÇ  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ         ‚ñº                                  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Normalization with Safety Belt       ‚îÇ  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ Concurrent Processing                      ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ThreadPoolExecutor (4 workers default)     ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                           ‚îÇ
       ‚ñº                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Supabase    ‚îÇ           ‚îÇ OpenAI      ‚îÇ
‚îÇ (PostgreSQL)‚îÇ           ‚îÇ Vector Store‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                           ‚ñ≤
       ‚îÇ                           ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ PAS Sidecar     ‚îÇ
              ‚îÇ (Background)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Architectural Components

#### 1. **Multi-Strategy PDF Processing** (app/gpt_extractor.py)

The system uses a sophisticated fallback strategy for PDF extraction:

```python
def call_gpt_extractor(document_id: str, pdf_bytes: bytes, cfg: Optional[GPTConfig] = None):
    # Strategy 1: Responses API with strict schema (preferred)
    try:
        payload = _responses_with_pdf(cfg.model, document_id, pdf_bytes, allow_schema=True)
        _SCHEMA_VALIDATOR.validate(payload)
        return payload
    except TypeError:
        pass  # SDK doesn't support response_format
    
    # Strategy 2: Responses API without schema validation
    try:
        payload = _responses_with_pdf(cfg.model, document_id, pdf_bytes, allow_schema=False)
        _SCHEMA_VALIDATOR.validate(payload)
        return payload
    except Exception:
        pass
    
    # Strategy 3: Chat Completions with extracted text (fallback)
    return _chat_with_text(model, document_id, pdf_bytes)
```

**Retry Logic**: Each strategy includes exponential backoff (0.7s √ó attempt).

#### 2. **Concurrent Processing Architecture** (app/main.py)
The backend is a modular FastAPI application. Each router module declares its own `APIRouter` with cohesive responsibilities and shared helpers. For example, the offers upload flow centralizes validation, duplicate detection, persistence, and OpenAI interaction in a single async handler, delegating vector store provisioning to the shared service layer:

```python
# Thread pool for parallel PDF processing
EXTRACT_WORKERS = int(os.getenv("EXTRACT_WORKERS", "4"))
EXEC: ThreadPoolExecutor = ThreadPoolExecutor(max_workers=EXTRACT_WORKERS)
_JOBS_LOCK = threading.Lock()

# Job tracking with thread safety
_jobs: Dict[str, Dict[str, Any]] = {}  # job_id -> status

def _process_pdf_bytes(data, doc_id, insurer, company, ...):
    # Runs in thread pool
    with _JOBS_LOCK:
        rec = _jobs.get(job_id)
        rec["done"] += 1
@router.post("/upload")
async def upload_offer_file(...):
    validate_environment()
    content = await file.read()
    validate_size(len(content))
    existing = check_duplicate(org_id, sha256)
    if existing:
        ...  # reattach to vector store
    batch_id = resolve_batch_id_by_token(batch_token, org_id)
    storage_path = save_to_storage(content, org_id, file.filename)
    retrieval_file_id = upload_to_openai(content, file.filename)
    attach_to_vector_store(vector_store_id, retrieval_file_id)
    return JSONResponse(payload)
```
„ÄêF:backend/api/routes/offers_upload.py‚Ä†L287-L592„Äë

**Pattern**: Submit jobs to executor, track progress in thread-safe dict, poll for completion.

#### 3. **State Management** (app/main.py)

   ```python
# In-memory caches (single-process only)
_LAST_RESULTS: Dict[str, Dict[str, Any]] = {}    # doc_id -> payload
_SHARES_FALLBACK: Dict[str, Dict[str, Any]] = {} # token -> share record
_INSERTED_IDS: Dict[str, List[int]] = {}         # doc_id -> DB row IDs
_jobs: Dict[str, Dict[str, Any]] = {}            # job_id -> status
```

**Issue**: Single-process state won't scale horizontally. Needs Redis/similar for multi-instance deployments.

#### 4. **Database Integration Pattern**

**Dual Storage Strategy**: Primary (Supabase) + In-Memory Fallback

```python
def save_to_supabase(payload: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
    doc_id = payload.get("document_id")
    _LAST_RESULTS[doc_id] = payload  # Always cache
    
    if not _supabase:
        return True, None  # Fallback mode
    
    try:
        rows = _rows_for_offers_table(payload)
        _supabase.table(_OFFERS_TABLE).insert(rows).execute()
        return True, None
    except Exception as e:
        return False, str(e)  # Graceful degradation
```

**Benefit**: System continues working even if Supabase is unavailable.  
**Risk**: Data loss if process crashes before database is restored.

#### 5. **Vector Store Architecture** (app/services/vectorstores.py)

```python
def ensure_offer_vs(conn, org_id, batch_token) -> str:
    """Lazy vector store creation per org √ó batch."""
    # Check cache table
    cur.execute("""SELECT vector_store_id FROM org_batch_vector_stores
                   WHERE org_id=%s AND batch_token=%s""", (org_id, batch_token))
    row = cur.fetchone()
    if row:
        return row["vector_store_id"]
    
    # Create new vector store
    vs_id = create_vector_store(client, name=f"org_{org_id}_offer_{batch_token}")
    
    # Cache it
    cur.execute("""INSERT INTO org_batch_vector_stores(org_id, batch_token, vector_store_id)
                   VALUES (%s,%s,%s)""", (org_id, batch_token, vs_id))
    return vs_id
```

**Pattern**: Lazy initialization with database-backed cache.

#### 6. **PAS Sidecar Pattern** (app/extensions/pas_sidecar.py)

**Post-Upload Asynchronous Processing**:

```python
# In main upload endpoint
background_tasks.add_task(run_batch_ingest_sidecar, org_id, batch_id)

# Sidecar runs after response returned
def run_batch_ingest_sidecar(org_id: int, batch_id: int):
    """Never throws - all errors caught and logged."""
    try:
        vector_store_id = ensure_batch_vector_store(org_id, batch_token)
        for file in files_to_process:
            add_file_to_batch_vs(vector_store_id, file_bytes, filename)
    except Exception as e:
        print(f"[sidecar] error: {e}")  # Log but don't crash
```

**Benefit**: Fast user response + background enrichment  
**Risk**: User sees success before vector processing completes

---
Routing favors explicit dependency injection (e.g., `Depends(get_db)`), ensuring transactional control and input validation through Pydantic models for Q&A payloads.„ÄêF:backend/api/routes/qa.py‚Ä†L20-L148„Äë Asynchronous workflows rely on OpenAI Assistants for retrieval augmented generation, with dual vector-store resolution (T&C + batch data) to compose responses.„ÄêF:backend/api/routes/qa.py‚Ä†L81-L148„Äë Maintenance tasks (batch cleanup, vector store seeding) reuse the same service abstractions, highlighting a layered architecture where scripts act as clients to the service layer.„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L85-L200„Äë„ÄêF:app/services/vectorstores.py‚Ä†L10-L166„Äë

## üé® Styling and UI

**Backend-Only Application**: No frontend UI components.

### API Response Formatting

**Consistent JSON Structure**:

```python
# Success pattern
{
    "ok": True,
    "document_id": "uuid::1::filename.pdf",
    "result": {
        "programs": [...],
        "warnings": [...],
        "_timings": {"total_s": 2.34}
    }
}

# Error pattern
{
    "detail": "Descriptive error message"
}
```

### OpenAPI Documentation

- **Auto-generated**: FastAPI provides Swagger UI at `/docs`
- **Tags**: Routes organized by tags (qa, batches, debug, etc.)
- **Examples**: Response models with Pydantic

### Logging Format

**Structured logging with prefixes**:

```python
print(f"[qa] chunks-report start share_token={share_token}")
print(f"[embedding] done file_id={file_id} chunks={count}")
print(f"[sidecar] vs-ready {vector_store_id}")
```

**Tags**: `[qa]`, `[embedding]`, `[sidecar]`, `[tc]`, `[upload]`

---
This backend-focused repository does not implement a visual UI. Styling concerns are limited to response payload structures and JSON formatting enforced via Pydantic validators and explicit serialization. Accessibility and theming are delegated to downstream clients consuming the API.„ÄêF:backend/api/routes/qa.py‚Ä†L25-L148„Äë„ÄêF:backend/api/routes/batches.py‚Ä†L21-L76„Äë

## ‚úÖ Code Quality and Testing

### Code Standards

#### **Strengths**

1. **Type Hints**: Comprehensive type annotations throughout
   ```python
   def _reembed_file(file_id: int, conn) -> dict:
   def _chunk_text(text: str, chunk_size: int = 1000) -> List[dict]:
   ```

2. **Pydantic Validation**: Strong request/response validation
   ```python
   class AskRequest(BaseModel):
       org_id: int
       batch_token: str
       product_line: str
       
       @_validator("product_line")
       def _pl_letters_only(cls, v: str) -> str:
           if not v or not v.isalpha():
               raise ValueError("product_line must contain only letters")
           return v.upper()
   ```

3. **Docstrings**: Most functions have descriptive docstrings
   ```python
   def _chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[dict]:
       """
       Split text into overlapping chunks.
       
       Args:
           text: Text to chunk
           chunk_size: Target size of each chunk in characters
           overlap: Number of characters to overlap between chunks
       
       Returns:
           List of dicts with 'text' and 'metadata' keys
       """
   ```

4. **Error Handling**: Custom exception classes
   ```python
   class ExtractionError(Exception):
       pass
   ```

#### **Weaknesses**

1. **No Linting Configuration**: No visible `.pylintrc`, `.flake8`, or `pyproject.toml`
   - No black/autopep8 formatting config
   - No mypy type checking config

2. **Print Statements in Production**: 100+ `print()` calls should use `logging` module
   ```python
   print(f"[embedding] start file_id={file_id}")  # Should be: logger.info(...)
   ```

3. **Broad Exception Handling**: 41+ instances of `except Exception:`
   ```python
   except Exception as e:  # Too broad - should catch specific exceptions
       raise HTTPException(status_code=500, detail=str(e))
   ```

4. **Debug Endpoints in Production**: `/debug/` routes not protected
   ```python
   @router.get("/debug/last-results")  # Should require authentication
   def debug_last_results():
       return _LAST_RESULTS  # Exposes internal state
   ```

### Testing

#### **Current Test Coverage**

```
backend/tests/
‚îú‚îÄ‚îÄ test_chunks_report.py      # Chunks report endpoint (252 lines)
‚îú‚îÄ‚îÄ test_reembed.py            # Re-embedding functionality (252 lines)
‚îî‚îÄ‚îÄ test_upload_smoke.py       # Upload smoke tests
```

**Test Frameworks**: pytest (inferred from test file naming)

**Test Quality**:
- ‚úÖ Unit tests with mocking
- ‚úÖ Integration tests
- ‚úÖ Manual testing helpers
- ‚ùå No coverage reports visible
- ‚ùå No CI/CD test runs
- ‚ùå Core extraction logic (`gpt_extractor.py`) untested

#### **Missing Tests**

1. **Critical Path**: PDF extraction pipeline (977 lines in `gpt_extractor.py`)
2. **Main API**: 90% of `app/main.py` endpoints untested
3. **Services**: Vector store operations, persistence layer
4. **Edge Cases**: Error handling, retry logic, fallbacks

### Documentation

#### **Excellent Documentation**

- ‚úÖ `CHUNKS_REPORT_API.md` - Complete API reference (399 lines)
- ‚úÖ `REEMBED_API.md` - Full re-embedding guide (451 lines)
- ‚úÖ `IMPLEMENTATION_SUMMARY.md` - Technical details
- ‚úÖ `QUICK_START.md` - Quick reference
- ‚úÖ OpenAPI docs auto-generated

#### **Missing Documentation**

- ‚ùå No README.md in project root
- ‚ùå No architecture diagrams
- ‚ùå No database schema documentation
- ‚ùå No deployment guide
- ‚ùå No troubleshooting FAQ

---
- **Linting/Formatting**: No dedicated configuration files (e.g., `pyproject.toml`, `.flake8`) are present; style consistency relies on developer discipline. Some modules contain unused imports (`urllib.parse.quote`), indicating linting gaps.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L1-L75„Äë
- **Type Annotations**: Python modules leverage typing hints selectively (e.g., helper signatures) but do not enforce mypy or runtime type checking.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L29-L204„Äë
- **Testing**: Pytest suites cover upload validation, re-embedding pipelines, and share chunk reporting helpers, using mocks for filesystem, OpenAI, and database dependencies.„ÄêF:backend/tests/test_upload_smoke.py‚Ä†L17-L106„Äë„ÄêF:backend/tests/test_reembed.py‚Ä†L1-L177„Äë„ÄêF:backend/tests/test_chunks_report.py‚Ä†L1-L138„Äë Coverage is partial: end-to-end scenarios depend on external services and environment variables, leading to conditional skips (e.g., missing `OPENAI_API_KEY`).
- **Documentation**: Inline docstrings and comments describe operational scripts and helper functions, but there is no centralized API reference or README for backend deployment beyond root quick-start files.„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L1-L200„Äë„ÄêF:backend/tests/test_reembed.py‚Ä†L143-L176„Äë

## üîß Key Components

### 1. **GPT Extractor** (`app/gpt_extractor.py` - 977 lines)

**Role**: Core PDF-to-structured-data extraction engine using OpenAI GPT models.

**Key Features**:
- Multi-strategy extraction with automatic fallback
- Strict JSON schema validation
- Multi-variant program detection
- Papildprogrammas (add-on programs) merging
- Normalizer safety belt to preserve extracted variants

**Usage Example**:

```python
from app.gpt_extractor import extract_offer_from_pdf_bytes

pdf_bytes = open("insurance_offer.pdf", "rb").read()
result = extract_offer_from_pdf_bytes(pdf_bytes, document_id="offer_123.pdf")

# Returns:
{
    "programs": [
        {
            "insurer": "BALTA",
            "program_code": "HEALTH-PLUS",
            "base_sum_eur": 50000,
            "premium_eur": 45.50,
            "features": {
                "Maksas stacionƒÅrie pakalpojumi, limits EUR": "500",
                ...
            }
        },
        ...
    ],
    "warnings": ["..."],
    "document_id": "offer_123.pdf"
}
```

**Dependencies**:
- `openai` - GPT API calls
- `pypdf` - PDF text extraction
- `jsonschema` - Schema validation
- `app.normalizer` - Data normalization

**API**:
- `extract_offer_from_pdf_bytes(pdf_bytes, document_id)` - Main entry point
- `call_gpt_extractor(document_id, pdf_bytes, cfg)` - Raw extraction
- `_responses_with_pdf()` - Responses API strategy
- `_chat_with_text()` - Chat Completions fallback
- `_augment_with_detected_variants()` - Variant synthesis
- `_merge_papild_into_programs()` - Add-on merging

---

### 2. **Main Application** (`app/main.py` - 1,396 lines)

**Role**: FastAPI application orchestrating all functionality.

**Key Features**:
- PDF upload endpoints (single, multiple, async)
- Job status tracking with `/jobs/{job_id}`
- Share token system for external access
- Template-based offer creation
- Batch management
- CORS configuration
- Context propagation (org_id, user_id via headers)

**Usage Example**:

```python
# Async batch upload
curl -X POST "http://localhost:8000/extract/multiple-async" \
  -H "X-Org-Id: 1" \
  -H "X-User-Id: 42" \
  -F "files=@offer1.pdf" \
  -F "files=@offer2.pdf" \
  -F "company=Acme Corp" \
  -F "insured_count=50"

# Returns immediately with job_id
{"job_id": "uuid-123", "accepted": 2, "documents": ["uuid::1::offer1.pdf", ...]}

# Poll status
GET /jobs/uuid-123
{"total": 2, "done": 2, "errors": [], "docs": [...]}
```

**Key Endpoints**:

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/extract/pdf` | POST | Single PDF extraction |
| `/extract/multiple` | POST | Batch extraction (sync) |
| `/extract/multiple-async` | POST | Batch extraction (async) |
| `/jobs/{job_id}` | GET | Job status polling |
| `/shares` | POST | Create share token |
| `/shares/{token}` | GET | Retrieve shared offers |
| `/offers/{offer_id}` | PATCH | Update offer |
| `/templates` | POST/GET | Template management |
| `/debug/*` | GET | Debug utilities |

**Dependencies**:
- `app.gpt_extractor` - PDF extraction
- `app.normalizer` - Data normalization
- `app.routes.*` - Modular route handlers
- `app.services.*` - Business logic
- `supabase` - Database operations

---

### 3. **Q&A Routes** (`backend/api/routes/qa.py` - 845 lines)

**Role**: OpenAI Assistants-based Q&A with vector search over offers and T&C documents.

**Key Features**:
- Multi-vector-store Q&A (batch + T&C)
- Chunks report endpoint (NEW)
- Re-embedding endpoint (NEW)
- Share token validation
- Authorization checks

**Usage Example**:

```python
# Ask question about offer batch
POST /api/qa/ask
{
    "org_id": 1,
    "batch_token": "bt_abc123",
    "product_line": "HEALTH",
    "asked_by_user_id": 42,
    "question": "Which insurer offers the best hospital coverage?"
}

# Returns top 3 insurers with scores
{
    "product_line": "HEALTH",
    "top3": [
        {
            "insurer_code": "BALTA",
            "score": 0.95,
            "reason": "Highest hospital limits with no co-pay",
            "sources": ["offer_balta.pdf", "tc_health_balta.pdf"]
        },
        ...
    ]
}
```

**Key Endpoints**:

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/qa/ask` | POST | Traditional Q&A (batch + T&C) |
| `/api/qa/ask-share` | POST | Q&A scoped to share token |
| `/api/qa/logs` | GET | Q&A history logs |
| `/api/qa/chunks-report` | GET | Retrieve document chunks |
| `/api/qa/reembed-file` | POST | Manual re-embedding |
| `/api/qa/seed-tc` | POST | Seed T&C vector store |

**Dependencies**:
- `openai` - OpenAI Assistants API
- `app.services.vectorstores` - Vector store management
- `pypdf` - PDF text extraction (for re-embedding)
- `psycopg2` - Direct database access

**Novel Features**:
- Compatibility wrapper for old vs. new OpenAI SDK versions
- Temporary assistants for per-request configuration
- Smart text chunking with boundary detection

---

### 4. **Vector Store Service** (`app/services/vectorstores.py` - 170 lines)

**Role**: Manage OpenAI vector stores for semantic search.

**Key Features**:
- Lazy vector store creation (org √ó product_line)
- Batch-specific vector stores (org √ó batch_token)
- T&C vector store seeding with canonical PDFs
- Database-backed caching

**Usage Example**:

```python
from app.services.vectorstores import ensure_offer_vs, get_tc_vs

# Get or create offer vector store
conn = psycopg2.connect(DATABASE_URL)
vs_id = ensure_offer_vs(conn, org_id=1, batch_token="bt_abc123")

# Retrieve T&C vector store
tc_vs_id = get_tc_vs(conn, org_id=1, product_line="HEALTH")
```

**API**:
- `ensure_tc_vs(conn, org_id, product_line)` - Get/create T&C store
- `ensure_offer_vs(conn, org_id, batch_token)` - Get/create batch store
- `get_tc_vs(conn, org_id, product_line)` - Retrieve T&C store ID
- `get_offer_vs(conn, org_id, batch_token)` - Retrieve batch store ID
- `ensure_tc_vector_store(org_id)` - Seed T&C store with PDFs

**Database Tables**:
```sql
org_vector_stores         -- org_id √ó product_line ‚Üí vector_store_id
org_batch_vector_stores   -- org_id √ó batch_token ‚Üí vector_store_id
```

---

### 5. **Normalizer** (`app/normalizer.py` - 259 lines)

**Role**: Standardize and validate extracted offer data.

**Key Features**:
- 52 canonical feature keys (insurance product features)
- Legacy key mapping for backward compatibility
- Type coercion (strings, numbers, euros)
- Program deduplication with meaningful suffixes
- Papildprogrammas (add-ons) folding into base programs

**Usage Example**:

```python
from app.normalizer import normalize_offer_json

raw_extraction = {
    "programs": [
        {
            "insurer": "BALTA",
            "program_code": "HEALTH-001",
            "base_sum_eur": "50000",
            "premium_eur": "45.50 EUR",
            "features": {
                "Maksas stacionƒÅrie pakalpojumi, limits EUR": 500,
                ...
            }
        }
    ]
}

normalized = normalize_offer_json(raw_extraction, document_id="offer_123.pdf")
# Ensures consistent data types, feature keys, and structure
```

**Key Functions**:
- `normalize_offer_json(raw, document_id)` - Main normalization
- `_unwrap(v)` - Extract values from dict wrappers
- `_coerce_feature_value(v)` - Standardize feature values
- `_coerce_base_sum(v)` - Parse euro amounts to integers
- `_disambiguate_duplicate_program_codes(rows)` - Add suffixes to duplicates

**Feature Keys** (sample):
- "Apdro≈°inƒÅjuma summa pamatpolisei, EUR"
- "Maksas stacionƒÅrie pakalpojumi, limits EUR"
- "Laborator iskie izmeklƒìjumi"
- "VakcinƒÅcija, limits EUR"
- "ZobƒÅrstniecƒ´ba ar 50% atlaidi, apdro≈°inƒÅjuma summa (pp)"

---
1. **Offers Upload Router (`backend/api/routes/offers_upload.py`)** ‚Äì Handles ingestion, deduplication, storage, and vector store attachment for uploaded documents. Depends on OpenAI, PostgreSQL, and optional S3; enforces MIME/size validation and batch scoping.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L287-L592„Äë
2. **QA Router (`backend/api/routes/qa.py`)** ‚Äì Provides multiple Q&A endpoints (Top-3 insurer ranking, share-scoped answers, chunk reporting, re-embedding). Integrates OpenAI Assistants with dynamic vector store selection and rigorous Pydantic validation.„ÄêF:backend/api/routes/qa.py‚Ä†L81-L608„Äë
3. **Batch Router (`backend/api/routes/batches.py`)** ‚Äì Creates and retrieves offer batches, assigning tokens and maintaining file associations for upload flows.„ÄêF:backend/api/routes/batches.py‚Ä†L21-L82„Äë
4. **Vector Store Service (`app/services/vectorstores.py`)** ‚Äì Shared layer encapsulating vector store creation, lookup, and seeding, supporting both batch and permanent T&C knowledge bases.„ÄêF:app/services/vectorstores.py‚Ä†L10-L166„Äë
5. **Cleanup Script (`backend/scripts/expire_and_cleanup_batches.py`)** ‚Äì Operational tool that decommissions expired batches, deleting OpenAI assets, local/S3 artifacts, and database records to prevent storage drift.„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L30-L200„Äë

## üß© Patterns and Best Practices

### 1. **Performance Optimizations**

#### Concurrent Processing
```python
EXEC: ThreadPoolExecutor = ThreadPoolExecutor(max_workers=EXTRACT_WORKERS)

for idx, file in enumerate(files):
    EXEC.submit(_process_pdf_bytes, data=data, doc_id=doc_id, ...)
```

**Impact**: 4√ó faster batch processing (default 4 workers).

#### Connection Pooling
```python
engine = create_engine(os.environ["DATABASE_URL"], future=True)
# SQLAlchemy manages connection pool automatically
```

#### Lazy Loading
```python
def ensure_offer_vs(conn, org_id, batch_token):
    # Check cache first
    row = cur.fetchone()
    if row:
        return row["vector_store_id"]
    # Create only if needed
    vs_id = create_vector_store(...)
```

#### In-Memory Caching
```python
_LAST_RESULTS: Dict[str, Dict[str, Any]] = {}  # Hot cache for recent results
_SHARES_FALLBACK: Dict[str, Dict[str, Any]] = {}  # Same-dyno share cache
```

**Trade-off**: Fast lookups but not horizontally scalable.

---

### 2. **Async Handling Patterns**

#### Background Tasks
```python
@app.post("/extract/multiple-async", status_code=202)
async def extract_multiple_async(request: Request, background_tasks: BackgroundTasks):
    # Return immediately
    background_tasks.add_task(run_batch_ingest_sidecar, org_id, batch_id)
    return {"job_id": job_id, "accepted": len(files)}
```

#### Thread Pool for CPU-Bound Work
```python
# Don't block async event loop with PDF processing
EXEC.submit(_process_pdf_bytes, data, doc_id, ...)
```

#### Polling Pattern
```python
# Client polls for job completion
@app.get("/jobs/{job_id}")
def job_status(job_id: str):
    with _JOBS_LOCK:
        job = _jobs.get(job_id)
    return job
```

**Alternative**: WebSocket/SSE would be more real-time but adds complexity.

---

### 3. **Error Handling & Resilience**

#### Multi-Strategy with Fallbacks
```python
# Try strategies in order until one succeeds
for attempt in range(cfg.max_retries + 1):
    try:
        return _responses_with_pdf(model, document_id, pdf_bytes, allow_schema=True)
except Exception as e:
        if attempt < cfg.max_retries:
            time.sleep(0.7 * (attempt + 1))  # Exponential backoff
```

#### Graceful Degradation
```python
def save_to_supabase(payload):
    _LAST_RESULTS[doc_id] = payload  # Always cache
    
    if not _supabase:
        return True, None  # Work without database
    
    try:
        _supabase.table(_OFFERS_TABLE).insert(rows).execute()
        return True, None
    except Exception as e:
        return False, str(e)  # Log but don't crash
```

#### Robust Share Token Loading
```python
def _load_share_record(token: str, attempts: int = 25, delay_s: float = 0.2):
    """Retry for ~5s to handle replication lag."""
    for i in range(max(1, attempts)):
        try:
            res = _supabase.table(_SHARE_TABLE).select("*").eq("token", token).execute()
            if res.data:
                return res.data[0]
        except Exception as e:
            if i + 1 < attempts:
                time.sleep(delay_s)
    # Fallback to in-memory cache
    return _SHARES_FALLBACK.get(token)
```

**Pattern**: Retry with delay, then fallback to cache.

---

### 4. **Validation Logic**

#### Pydantic Validators
```python
class AskRequest(BaseModel):
    product_line: str
    
    @_validator("product_line")
    def _pl_letters_only(cls, v: str) -> str:
        if not v or not v.isalpha():
            raise ValueError("product_line must contain only letters")
        return v.upper()
```

#### JSON Schema Validation
```python
INSURER_OFFER_SCHEMA = {
    "type": "object",
    "properties": {
        "programs": {"type": "array", "items": {...}},
        "warnings": {"type": "array"}
    },
    "required": ["programs"],
    "additionalProperties": False
}

_SCHEMA_VALIDATOR = Draft202012Validator(INSURER_OFFER_SCHEMA)
_SCHEMA_VALIDATOR.validate(payload)  # Raises ValidationError if invalid
```

#### Custom Validation Functions
```python
def _num(v: Any) -> Optional[float]:
    """Best-effort numeric coercion. Returns None for blanks, dashes, N/A."""
    if v is None:
        return None
    if isinstance(v, str):
        s = v.strip()
        if s in {"", "-", "‚Äì", "‚Äî", "n/a", "N/A", "NA"}:
            return None
        s = s.replace(" ", "").replace(",", ".")
        # Handle European number format (1.000,50 ‚Üí 1000.50)
        if s.count('.') > 1:
            head, _, tail = s.rpartition('.')
            head = head.replace('.', '')
            s = f"{head}.{tail}"
        try:
            return float(s)
        except Exception:
            return None
    return None
```

---

### 5. **Code Reuse Patterns**

#### Dependency Injection with FastAPI
```python
def get_db():
    conn = psycopg2.connect(os.getenv("DATABASE_URL"), cursor_factory=RealDictCursor)
    try:
        yield conn
    finally:
        conn.close()

@router.post("/reembed-file")
def reembed_file(file_id: int, conn = Depends(get_db)):
    # 'conn' automatically injected and cleaned up
```

#### Context Propagation Helper
```python
def _ctx_ids(request: Optional[Request]) -> Tuple[Optional[int], Optional[int]]:
    """Extract org_id and user_id from request headers."""
    org = request.headers.get("X-Org-Id")
    usr = request.headers.get("X-User-Id")
    return (int(org) if org else None), (int(usr) if usr else None)

# Used everywhere
org_id, user_id = _ctx_ids(request)
```

#### Modular Route Inclusion
```python
app.include_router(offers_by_documents_router)
app.include_router(debug_db_router)
app.include_router(qa_router)
app.include_router(batches_router)
```

#### Shared Utilities Module
```python
# app/services/openai_compat.py
def create_vector_store(client, name: str) -> str:
    """Cross-version compatible vector store creation."""
    
# Used by multiple services
from app.services.openai_compat import create_vector_store
```

---
- **Transactional Context Managers**: Database operations consistently use context managers to ensure cursors close and transactions commit/rollback deterministically.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L171-L592„Äë„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L95-L200„Äë
- **Compatibility Guards**: OpenAI integration layers include fallbacks between stable and beta SDK namespaces, future-proofing against API changes.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L258-L284„Äë„ÄêF:app/services/openai_compat.py‚Ä†L5-L34„Äë
- **Idempotent Operations**: Duplicate upload handling reattaches existing files and self-heals missing storage paths, preventing redundant OpenAI uploads.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L314-L474„Äë
- **Validation & Logging**: Pydantic validators enforce strict payloads, while extensive logging aids observability across upload, QA, and script workflows.„ÄêF:backend/api/routes/qa.py‚Ä†L25-L608„Äë„ÄêF:backend/api/routes/offers_upload.py‚Ä†L297-L592„Äë

## ‚öôÔ∏è Development Infrastructure

### package.json Scripts

```json
{
  "scripts": {
    "create:vector-stores": "tsx backend/scripts/create-vector-stores.ts",
    "dev": "uvicorn app.main:app --reload --host 0.0.0.0 --port 8000",
    "start": "uvicorn app.main:app --host 0.0.0.0 --port 8000"
  }
}
```

**Usage**:
```bash
npm run dev       # Development with hot reload
npm start         # Production server
```

### Makefile Commands

```makefile
cleanup-batches:
    python backend/scripts/expire_and_cleanup_batches.py

create-vector-store:
    export ORG_ID=$(ORG_ID) && python backend/scripts/create_vector_store.py
```

**Usage**:
```bash
make cleanup-batches                  # Expire old batches
make create-vector-store ORG_ID=1     # Initialize vector store
```

### Environment Configuration

**Required Variables**:
```bash
DATABASE_URL=postgresql://user:pass@host:5432/db
OPENAI_API_KEY=sk-...
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_ANON_KEY=eyJ...
SUPABASE_SERVICE_ROLE_KEY=eyJ...
```

**Optional Variables**:
```bash
# Processing
EXTRACT_WORKERS=4                    # Thread pool size
GPT_MODEL=gpt-4o-mini               # Default extraction model
FALLBACK_CHAT_MODEL=gpt-4o-mini     # Fallback model
KEEP_SYNTH_MULTI=1                  # Keep synthesized variants

# Storage
STORAGE_ROOT=/tmp                    # File storage root
BATCH_TTL_DAYS=30                   # Batch expiration

# Organization defaults
DEFAULT_ORG_ID=1                    # Default org for legacy endpoints
DEFAULT_USER_ID=1                   # Default user for legacy endpoints

# Assistants
ASSISTANT_ID_TOP3=asst_...          # Top-3 ranking assistant
ASSISTANT_ID_QA=asst_...            # Q&A assistant
ASSISTANT_MODEL=gpt-4.1-mini        # Assistant model

# Share links
SHARE_BASE_URL=https://vis.ongo.lv  # Share URL prefix
```

### Docker Configuration

```dockerfile
FROM python:3.11-slim
WORKDIR /app
RUN apt-get update && apt-get install -y build-essential
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["sh","-c","uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}"]
```

**Features**:
- ‚úÖ Python 3.11 slim base (minimal size)
- ‚úÖ Build tools for compiled dependencies
- ‚úÖ No cache pip install (smaller image)
- ‚úÖ Dynamic port via `${PORT}` env var

**Build & Run**:
```bash
docker build -t gpt-offer-extractor .
docker run -p 8000:8000 --env-file .env gpt-offer-extractor
```

### CI/CD

**Not Visible**: No `.github/workflows/`, `.gitlab-ci.yml`, or similar detected.

**Recommendations**:
- Add GitHub Actions for tests on PR
- Add Docker image build/push
- Add deployment automation
- Add security scanning (Snyk, Dependabot)

### Pre-commit Hooks

**Not Detected**: No `.pre-commit-config.yaml` or `.husky/` found.

**Recommendations**:
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.3.0
    hooks:
      - id: mypy
```

---
- **Python dependencies** pinned via `requirements.txt`, emphasizing reproducible OpenAI and database clients.„ÄêF:requirements.txt‚Ä†L1-L10„Äë
- **Node/TypeScript tooling** defined in `package.json` for vector store provisioning and ASGI server startup, mixing ecosystems for operational flexibility.„ÄêF:package.json‚Ä†L6-L21„Äë
- **Makefile** shortcuts orchestrate cleanup and vector store creation scripts, requiring environment variables for org scoping.„ÄêF:Makefile‚Ä†L1-L9„Äë
- **Environment Configuration**: Runtime expectations rely on environment variables (`DATABASE_URL`, `OPENAI_API_KEY`, `S3_BUCKET`, assistant IDs) checked at runtime rather than through `.env` scaffolding. Validation helpers exist for critical paths but not uniformly across modules.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L82-L89„Äë„ÄêF:backend/api/routes/qa.py‚Ä†L191-L284„Äë
- **Dockerfile/CI**: No CI configuration or Dockerfile dedicated to the backend is present in this tree, suggesting deployment responsibilities lie elsewhere or remain manual.

## ‚ö†Ô∏è Bug & Issue Report

### üî¥ CRITICAL ISSUES

#### 1. **CORS Configuration Security Risk**
- **File**: `app/main.py:127`
- **Line**: 129
- **Code**:
  ```python
  app.add_middleware(
      CORSMiddleware,
      allow_origins=["*"],  # ‚Üê CRITICAL SECURITY ISSUE
      allow_credentials=True,
      ...
  )
  ```
- **Problem**: Wildcard CORS origin (`["*"]`) with `allow_credentials=True` is a **severe security vulnerability**. Allows any website to make authenticated requests to the API, enabling CSRF attacks and credential theft.
- **Risk**: üî¥ **CRITICAL** - Enables cross-origin attacks in production
- **Suggested Fix**:
  ```python
  ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "https://vis.ongo.lv").split(",")
  app.add_middleware(
      CORSMiddleware,
      allow_origins=ALLOWED_ORIGINS,
      allow_credentials=True,
      ...
  )
  ```

#### 2. **Debug Endpoints Exposed in Production**
- **Files**: `app/main.py:1373-1395`, `app/routes/debug_db.py`
- **Endpoints**:
  - `GET /debug/last-results` - Exposes `_LAST_RESULTS` dict (all extracted data)
  - `GET /debug/doc/{doc_id}` - Exposes any document by ID
  - `GET /debug/db-info` - Exposes database connection details
- **Problem**: No authentication required. Anyone can access internal state and database information.
- **Risk**: üî¥ **CRITICAL** - Information disclosure, privacy violation
- **Suggested Fix**:
  ```python
  # Add environment check
  if os.getenv("ENV") != "development":
      # Don't include debug router in production
      pass
  else:
      app.include_router(debug_db_router)
  
  # OR add authentication
  @router.get("/debug/last-results", dependencies=[Depends(verify_admin)])
  ```

---

### üü† HIGH PRIORITY ISSUES

#### 3. **Single-Process State Management**
- **File**: `app/main.py:155-158`
- **Code**:
  ```python
  _jobs: Dict[str, Dict[str, Any]] = {}
  _LAST_RESULTS: Dict[str, Dict[str, Any]] = {}
  _SHARES_FALLBACK: Dict[str, Dict[str, Any]] = {}
  _INSERTED_IDS: Dict[str, List[int]] = {}
  ```
- **Problem**: In-memory state doesn't work with:
  - Multiple server instances (horizontal scaling)
  - Server restarts (data loss)
  - Load balancers (inconsistent state across instances)
- **Risk**: üü† **HIGH** - Scaling limitation, data loss risk
- **Suggested Fix**:
  ```python
  # Use Redis for shared state
  import redis
  redis_client = redis.Redis.from_url(os.getenv("REDIS_URL"))
  
  def get_job_status(job_id):
      data = redis_client.get(f"job:{job_id}")
      return json.loads(data) if data else None
  ```

#### 4. **No Logging Framework**
- **Files**: Throughout codebase (100+ instances)
- **Code**:
  ```python
  print(f"[embedding] start file_id={file_id}")  # üò±
  ```
- **Problem**:
  - No log levels (DEBUG, INFO, WARNING, ERROR)
  - No structured logging (can't parse/filter)
  - No log rotation or management
  - Performance impact (synchronous I/O)
- **Risk**: üü† **HIGH** - Production debugging difficulty, performance impact
- **Suggested Fix**:
  ```python
  import logging
  
  logger = logging.getLogger(__name__)
  logging.basicConfig(
      level=logging.INFO,
      format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
  )
  
  logger.info("embedding start file_id=%s", file_id)
  ```

#### 5. **Broad Exception Handling (41+ instances)**
- **Files**: Throughout codebase
- **Examples**:
  ```python
  except Exception as e:  # Too broad!
      raise HTTPException(status_code=500, detail=str(e))
  ```
- **Problem**:
  - Masks specific errors (KeyError, ValueError, etc.)
  - Makes debugging harder
  - Can hide programming errors
- **Risk**: üü† **HIGH** - Debugging difficulty, hidden bugs
- **Suggested Fix**:
  ```python
  except (ValueError, KeyError) as e:
      # Handle expected errors
  except psycopg2.DatabaseError as e:
      # Handle database errors
  except Exception as e:
      # Only for truly unexpected errors
      logger.exception("Unexpected error")
      raise
  ```

#### 6. **Pinned OpenAI Version (2.2.0)**
- **File**: `requirements.txt:4`
- **Code**: `openai==2.2.0         # üîí pin here (critical!)`
- **Problem**: Version 2.2.0 released May 2024 - now 5+ months old. Missing:
  - Security patches
  - Bug fixes
  - Performance improvements
  - New features
- **Risk**: üü† **HIGH** - Security vulnerabilities, missing bug fixes
- **Suggested Fix**:
  - Test with latest version (4.x series)
  - Document compatibility issues if any
  - Update gradually: `openai>=2.2.0,<3.0.0` then `<4.0.0` then `<5.0.0`

---

### üü° MEDIUM PRIORITY ISSUES

#### 7. **TODO Comments in Production Code**
- **File**: `backend/api/routes/offers_upload.py:154, 160, 495, 499`
- **Code**:
  ```python
  def _scan_for_viruses(content: bytes) -> None:
      """TODO: Implement antivirus scanning."""
      pass  # Not implemented!
  
  def _redact_pii(content: bytes) -> bytes:
      """TODO: Implement PII redaction."""
      return content  # Returns unchanged!
  ```
- **Problem**: Security features marked as TODO but not implemented. Functions are called but do nothing.
- **Risk**: üü° **MEDIUM** - Security gaps, false sense of security
- **Suggested Fix**:
  - Implement ClamAV integration for virus scanning
  - Implement PII redaction or remove the calls
  - Add `NotImplementedError` if not ready for production:
    ```python
    raise NotImplementedError("Virus scanning not yet implemented")
    ```

#### 8. **Dual psycopg Versions**
- **File**: `requirements.txt:9-10`
- **Code**:
  ```python
  psycopg[binary]==3.2.1
  psycopg2-binary==2.9.9
  ```
- **Problem**: Both psycopg2 and psycopg3 installed. Indicates incomplete migration.
- **Risk**: üü° **MEDIUM** - Dependency bloat, confusion
- **Suggested Fix**:
  - Complete migration to psycopg3 (modern, async-capable)
  - OR stick with psycopg2 and remove psycopg3
  - Don't ship both in production

#### 9. **No Request ID Tracing**
- **Problem**: Can't correlate logs across distributed processing
- **Example**:
  ```
  [embedding] start file_id=46       # Which request?
  [qa] chunks-report start           # Related to what?
  ```
- **Risk**: üü° **MEDIUM** - Difficult to trace requests through system
- **Suggested Fix**:
  ```python
  import uuid
  from fastapi import Request
  
  @app.middleware("http")
  async def add_request_id(request: Request, call_next):
      request_id = str(uuid.uuid4())
      request.state.request_id = request_id
      response = await call_next(request)
      response.headers["X-Request-Id"] = request_id
      return response
  
  # In logging
  logger.info("start file_id=%s request_id=%s", file_id, request.state.request_id)
  ```

#### 10. **No Database Migration Tool**
- **Files**: `backend/scripts/create_offer_chunks_table.sql`
- **Problem**: SQL files but no migration management (Alembic, Flyway, etc.)
  - No version tracking
  - No rollback capability
  - Manual application required
- **Risk**: üü° **MEDIUM** - Schema drift, deployment issues
- **Suggested Fix**:
  ```bash
  # Use Alembic
  pip install alembic
  alembic init alembic
  alembic revision -m "create offer_chunks table"
  alembic upgrade head
  ```

#### 11. **No Health Check Logic**
- **File**: `app/main.py:181-192`
- **Code**:
  ```python
  @app.get("/healthz")
  def healthz():
      return {"ok": True, ...}  # Always returns ok!
  ```
- **Problem**: Health check doesn't verify:
  - Database connectivity
  - OpenAI API accessibility
  - Disk space availability
  - Critical services status
- **Risk**: üü° **MEDIUM** - Load balancer may route to unhealthy instances
- **Suggested Fix**:
  ```python
  @app.get("/healthz")
  def healthz():
      checks = {
          "database": _check_db(),
          "openai": _check_openai(),
          "storage": _check_storage()
      }
      all_ok = all(checks.values())
      status_code = 200 if all_ok else 503
      return JSONResponse({"ok": all_ok, "checks": checks}, status_code=status_code)
  ```

#### 12. **Hardcoded Sleep Timers**
- **File**: `backend/api/routes/qa.py:321`
- **Code**:
  ```python
  time.sleep(1)  # Poll every second
  ```
- **Problem**: Fixed 1-second polling is inefficient
  - Too slow for fast operations
  - Too fast for long operations (wastes CPU)
- **Risk**: üü° **MEDIUM** - Performance inefficiency
- **Suggested Fix**:
  ```python
  # Exponential backoff
  wait_time = 0.1
  while wait_time < max_wait:
      r = client.beta.threads.runs.retrieve(...)
      if r.status in ("completed", "failed", ...):
          break
      time.sleep(wait_time)
      wait_time = min(wait_time * 1.5, 5.0)  # Cap at 5 seconds
  ```

---

### üîµ LOW PRIORITY ISSUES

#### 13. **Inconsistent Naming Conventions**
- **Examples**:
  - `get_db()` vs `get_db_connection()`
  - `ensure_tc_vs()` vs `ensure_offer_vs()` (abbreviation inconsistency)
  - `_ctx_ids()` vs `_ctx_or_defaults()` (prefix inconsistency)
- **Risk**: üîµ **LOW** - Code readability
- **Suggested Fix**: Establish naming guidelines

#### 14. **Magic Numbers**
- **Examples**:
  ```python
  if len(text.strip()) < 10:  # Why 10?
  chunk_size=1000, overlap=200  # Why these values?
  max_wait = 60  # Why 60 seconds?
  ```
- **Risk**: üîµ **LOW** - Maintainability
- **Suggested Fix**: Use named constants
  ```python
  MIN_TEXT_LENGTH = 10
  DEFAULT_CHUNK_SIZE = 1000
  MAX_POLLING_TIMEOUT_SECONDS = 60
  ```

#### 15. **Missing Type Hints in Some Functions**
- **Example**: `app/main.py:243`
  ```python
  def _inject_meta(payload, *, insurer, company, insured_count, inquiry_id):
      # No type hints on parameters
  ```
- **Risk**: üîµ **LOW** - Type safety
- **Suggested Fix**: Add type hints consistently
  ```python
  def _inject_meta(
      payload: Dict[str, Any], 
      *, 
      insurer: str, 
      company: str, 
      insured_count: int, 
      inquiry_id: str
  ) -> None:
  ```

#### 16. **Commented-Out Code**
- **File**: `app/normalizer.py` (multiple locations)
- **Problem**: Old commented code clutters the file
- **Risk**: üîµ **LOW** - Code cleanliness
- **Suggested Fix**: Remove commented code (it's in git history)

#### 17. **Long Functions**
- **Examples**:
  - `app/main.py:extract_multiple_async()` - 90+ lines
  - `backend/api/routes/qa.py:ask_share_qa()` - 180+ lines
- **Problem**: Hard to test, understand, and maintain
- **Risk**: üîµ **LOW** - Maintainability
- **Suggested Fix**: Extract helper functions
  ```python
  def ask_share_qa(req, conn):
      share_record = _validate_share(req.share_token, conn)
      vector_stores = _get_vector_stores(share_record, conn)
      result = _execute_query(req.question, vector_stores)
      return _format_response(result)
  ```

---
- **File**: `backend/api/routes/batches.py` ‚Äì Returning raw `datetime` objects inside a `JSONResponse` causes serialization failures (`TypeError: Object of type datetime is not JSON serializable`). Convert timestamps to ISO strings before returning.„ÄêF:backend/api/routes/batches.py‚Ä†L47-L76„Äë
  - **Suggested Fix**: Pass data through FastAPI's `jsonable_encoder` or manually call `.isoformat()` on datetime fields prior to constructing the response.
- **File**: `backend/api/routes/offers_upload.py` & `requirements.txt` ‚Äì S3 upload paths import `boto3`, but the dependency is not declared; deployments enabling `S3_BUCKET` will crash at import time.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L211-L224„Äë„ÄêF:requirements.txt‚Ä†L1-L10„Äë
  - **Suggested Fix**: Add `boto3` to Python dependencies or guard the import with an informative runtime error.
- **File**: `backend/tests/test_upload_smoke.py` ‚Äì Test posts to `/api/offers/upload` without the now-required `batch_token`, so it receives HTTP 422 instead of 200, causing false failures.„ÄêF:backend/tests/test_upload_smoke.py‚Ä†L38-L47„Äë„ÄêF:backend/api/routes/offers_upload.py‚Ä†L482-L493„Äë
  - **Suggested Fix**: Include a valid `batch_token` (mocked or fixture-provided) in the test payload or adjust expectations for 422 responses.

## üìã Summary & Recommendations

### üåü Key Strengths

1. **Sophisticated AI Integration**: Multi-strategy PDF extraction with automatic fallback demonstrates production-ready AI engineering.

2. **Comprehensive Feature Set**: 
   - PDF extraction pipeline
   - Vector search with OpenAI Assistants
   - Batch processing with job tracking
   - Share token system for external access
   - Template management
   - Manual re-embedding tools

3. **Good Type Safety**: Extensive use of type hints and Pydantic validation.

4. **Excellent Documentation**: 1,500+ lines of API documentation across multiple markdown files.

5. **Dual API Design**: V1 (monolithic) and V2 (modular) coexist, allowing gradual migration.

6. **Resilient Architecture**: Fallback strategies, retry logic, graceful degradation.

7. **Developer Experience**: OpenAPI docs, CLI tools, test suite, helpful logging prefixes.

### ‚ö†Ô∏è Critical Weaknesses

1. **üî¥ Security Vulnerabilities**:
   - Wildcard CORS with credentials (CRITICAL)
   - Debug endpoints exposed without authentication (CRITICAL)
   - TODO security features not implemented (MEDIUM)

2. **üü† Scalability Limitations**:
   - In-memory state doesn't scale horizontally
   - No Redis or distributed cache
   - Single-process job tracking

3. **üü† Production Readiness**:
   - No proper logging framework (print statements)
   - No monitoring/observability setup
   - No health checks with actual verification
   - No CI/CD pipeline

4. **üü° Code Quality**:
   - 41+ broad exception handlers
   - 100+ print() statements
   - No linting/formatting configuration
   - Incomplete test coverage (<20% estimated)

5. **üü° Technical Debt**:
   - Dual psycopg versions (migration incomplete)
   - Pinned OpenAI version (5 months old)
   - Monolithic `app/main.py` (1,396 lines)

### üìä Complexity Assessment

**Level**: **Mid-to-Senior Level**

**Reasoning**:
- **AI/ML Integration**: Requires understanding of OpenAI APIs, embeddings, vector search
- **Concurrent Processing**: ThreadPoolExecutor, async/await patterns
- **Complex Business Logic**: Multi-variant extraction, normalization, schema validation
- **Database Design**: Vector stores, batch management, share tokens
- **Error Handling**: Multiple fallback strategies, retry logic
- **Architecture**: Dual-track development, sidecar pattern, event-driven background tasks

**Skills Required**:
- Python 3.11+ (type hints, async)
- FastAPI framework
- PostgreSQL + Supabase
- OpenAI API (GPT-4, Assistants, Vector Stores)
- PDF processing
- Concurrent programming
- RESTful API design

### üéØ Recommended Improvements

#### Immediate (This Sprint)

1. **üî¥ FIX CRITICAL SECURITY ISSUES**
   ```python
   # CORS fix
   ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "https://vis.ongo.lv").split(",")
   app.add_middleware(CORSMiddleware, allow_origins=ALLOWED_ORIGINS, ...)
   
   # Disable debug endpoints in production
   if os.getenv("ENV") != "development":
       # Don't include debug router
       pass
   ```

2. **üü† Implement Proper Logging**
   ```python
   import logging
   logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
   logger = logging.getLogger(__name__)
   ```

3. **üü† Add Health Checks**
   ```python
   @app.get("/healthz")
   def healthz():
       checks = {"database": check_db(), "openai": check_openai()}
       return {"ok": all(checks.values()), "checks": checks}
   ```

#### Short-term (Next Sprint)

4. **Add CI/CD Pipeline**
   ```yaml
   # .github/workflows/test.yml
   - name: Run tests
     run: pytest backend/tests/ -v --cov=app --cov=backend
   ```

5. **Implement Request ID Tracing**
   ```python
   @app.middleware("http")
   async def add_request_id(request, call_next):
       request_id = str(uuid.uuid4())
       request.state.request_id = request_id
       ...
   ```

6. **Add Linting Configuration**
   ```bash
   pip install black flake8 mypy
   black app backend
   flake8 app backend
   mypy app backend
   ```

7. **Expand Test Coverage** to 60%+
   - Add tests for `gpt_extractor.py` (0% currently)
   - Add tests for `main.py` endpoints (10% currently)
   - Add integration tests for end-to-end flows

#### Medium-term (Next Month)

8. **Migrate to Redis for State Management**
   ```python
   redis_client = redis.Redis.from_url(os.getenv("REDIS_URL"))
   # Replace all in-memory dicts
   ```

9. **Implement Database Migrations**
   ```bash
   alembic init alembic
   alembic revision -m "initial schema"
   ```

10. **Update Dependencies**
    - Test with OpenAI 4.x
    - Remove psycopg2 (use only psycopg3)
    - Update all dependencies to latest stable

11. **Add Monitoring & Observability**
    - Sentry for error tracking
    - Datadog/Prometheus for metrics
    - ELK stack for log aggregation

#### Long-term (Next Quarter)

12. **Refactor Monolithic Files**
    - Break down `app/main.py` (1,396 lines ‚Üí ~300 lines)
    - Extract services to separate modules
    - Complete V1 ‚Üí V2 API migration

13. **Implement Rate Limiting**
    ```python
    from slowapi import Limiter
    limiter = Limiter(key_func=get_remote_address)
    @app.post("/extract/pdf")
    @limiter.limit("10/minute")
    ```

14. **Add Caching Layer**
    - Redis for API response caching
    - CDN for static assets
    - Vector store result caching

15. **Security Audit**
    - Implement OAuth2/JWT authentication
    - Add API key management
    - Implement PII redaction (currently TODO)
    - Add virus scanning (currently TODO)

### üìà Success Metrics

**Track these after improvements**:

1. **Security**: 0 critical vulnerabilities (currently 2)
2. **Test Coverage**: >80% (currently ~20%)
3. **Response Time**: P95 < 500ms for non-PDF endpoints
4. **Error Rate**: <0.1% (currently unknown)
5. **Uptime**: >99.9% (currently unknown)

### üèÅ Overall Assessment

**Rating**: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5 stars)

**Verdict**: **Production-Capable with Critical Fixes Required**

The codebase demonstrates strong engineering fundamentals with sophisticated AI integration, good type safety, and comprehensive features. However, **critical security issues must be addressed before production deployment**.

**Strengths Outweigh Weaknesses**: The architecture is sound, the documentation is excellent, and the feature set is comprehensive. With the recommended fixes (particularly security issues), this becomes a **solid production-grade system**.

**Next Steps**:
1. ‚úÖ Fix CORS configuration (5 minutes)
2. ‚úÖ Disable debug endpoints in production (10 minutes)
3. ‚úÖ Implement proper logging (1 hour)
4. ‚úÖ Add health checks (1 hour)
5. ‚úÖ Deploy with fixes applied

**Estimated Time to Production-Ready**: **1-2 days** for critical fixes, **2-4 weeks** for all recommended improvements.

---

*Report Generated: October 26, 2025*  
*Codebase Version: 1.0.0*  
*Analysis Scope: Complete backend codebase (7,500+ lines)*
**Strengths**
- Well-structured FastAPI routers with rich validation, logging, and error handling around critical flows (uploads, QA, maintenance).„ÄêF:backend/api/routes/offers_upload.py‚Ä†L287-L592„Äë„ÄêF:backend/api/routes/qa.py‚Ä†L81-L608„Äë
- Robust service abstractions for vector store lifecycle management, including compatibility helpers and bulk seeding scripts.„ÄêF:app/services/vectorstores.py‚Ä†L10-L166„Äë„ÄêF:backend/scripts/create-vector-stores.ts‚Ä†L1-L91„Äë
- Comprehensive operational tooling (cleanup, re-embed) and targeted unit tests for complex routines (chunking, PDF extraction).„ÄêF:backend/scripts/expire_and_cleanup_batches.py‚Ä†L30-L200„Äë„ÄêF:backend/tests/test_reembed.py‚Ä†L1-L177„Äë

**Weaknesses**
- Missing dependency declarations (e.g., `boto3`) and serialization oversights introduce runtime instability under common configurations.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L211-L224„Äë„ÄêF:backend/api/routes/batches.py‚Ä†L47-L76„Äë
- Lack of automated linting/typing pipelines allows unused imports and inconsistent typing patterns to linger.„ÄêF:backend/api/routes/offers_upload.py‚Ä†L1-L75„Äë
- Test coverage depends on real OpenAI credentials and outdated payloads, limiting reliability of automated suites.„ÄêF:backend/tests/test_upload_smoke.py‚Ä†L12-L87„Äë

**Recommendations**
1. Address identified bugs (datetime serialization, dependency declarations, test payloads) to stabilize critical endpoints and CI.
2. Introduce tooling (e.g., Ruff/Black, mypy) and CI workflows to enforce style and catch regressions earlier.
3. Expand test fixtures/mocks to decouple from external APIs, enabling deterministic CI without secret configuration.
4. Document expected environment variables and deployment steps (Docker/compose) to streamline onboarding.

Overall complexity leans **mid-to-senior friendly**: domain logic and integrations are sophisticated (OpenAI Assistants, vector stores, batch lifecycle), but modules are well-organized with clear contracts. A mid-level engineer experienced with FastAPI and external APIs should navigate and extend the system effectively once environment prerequisites are clarified.